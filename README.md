3目並べのプログラムを使い、強化学習で自己対戦を繰り返すことで強くなるプログラムを作成する。
下記をベースとして使用する。
https://qiita.com/ogahiro21/items/bbe5052c3be215983096


## 全体像

* **対象ゲーム**：三目並べ（3×3）
* **学習方法**：自己対戦による強化学習（Qテーブル）
* **最終目的**：

  1. コンピュータ同士で大量に対戦して学習
  2. 学習後、人間と対戦できるようにする

---

## ① ゲームの基本ロジック

### 勝敗判定・盤面操作

* `WIN_LINES`
  → 勝利となるマスの組み合わせ定義
* `check_winner(board)`
  → 勝者（"X" or "O"）を判定
* `is_full(board)`
  → 盤面が埋まったか確認
* `empty_cells(board)`
  → 空いているマス一覧
* `display_board(board)`
  → コンソールに盤面を表示

👉 **純粋な三目並べのルール処理**

---

## ② 強化学習エージェント（SelfPlayAgent）

### Qテーブル

```python
self.q = defaultdict(lambda: defaultdict(float))
```

* 状態 → 行動 → 価値(Q値) を保持
* ニューラルネットではなく **テーブル型Q学習**

---

### 状態表現（重要）

```python
def encode_state(self, board, player):
```

盤面を

* 自分の石 → `"P"`
* 相手の石 → `"E"`
* 空き → `"_"`

に変換して **視点依存の状態** を作っています。

👉

* X視点 / O視点で同じ盤面を別状態として扱う
* 状態数を減らし、学習を安定させる工夫

---

### 行動選択（ε-greedy）

```python
def choose_action(self, board, player, explore=True):
```

* εの確率でランダム行動（探索）
* それ以外はQ値最大の手を選択（活用）

👉 **典型的な強化学習の探索戦略**

---

### Q値更新

```python
def update(self, state, action, reward, next_state, done):
```

更新式（少し特徴あり）：

```text
Q(s,a) ← Q(s,a) + α [ (reward − γ max Q(s')) − Q(s,a) ]
```

※ 一般的な
`reward + γ maxQ(s')`
ではなく、**符号が逆** になっている点が特徴です。

👉

* 相手番を挟む「自己対戦」を簡略化するための実装
* 実験・学習用としては成立する構造

---

## ③ 自己対戦による学習

```python
def train(self, episodes=40000):
```

* X と O を交互に担当
* 勝てば `reward = 1.0`
* 負け・引き分けは報酬なし
* 数万回の自己対戦でQテーブルを構築

👉 **教師データなしでルールを学習**

---

## ④ 人間 vs コンピュータ対戦

```python
def play_against_agent(agent, episodes):
```

流れ：

1. 自己対戦で学習
2. 人間が先手 or 後手を選択
3. コンピュータは **探索なし（最善手）**
4. コンソール上で対戦

---

## ⑤ まとめ（このプログラムは何をしている？）

✔ 三目並べを
✔ **自己対戦による強化学習**で
✔ 勝ちやすい手を学習し
✔ **人間と対戦できるAI** を作るプログラム

---

## 補足（学習教材としての価値）

このコードは特に、

* 強化学習の最小構成
* Qテーブルの考え方
* 状態設計の工夫
* 自己対戦の基本構造

を理解するのに **非常に良い教材** です。
